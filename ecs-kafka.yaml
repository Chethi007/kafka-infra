---
AWSTemplateFormatVersion: '2010-09-09'

Description: >
  Creates an ECS cluster for core Kafka components, Zookeeper and Brokers. It expects data volumes for each ECS
  Instance to be precreated and tagged. The cluster is spread across 3 availability zones with one autoscaling
  group per availability zone.

Parameters:
  # Networking
  KeyName:
    Description: Name of an existing EC2 KeyPair to enable SSH access to the ECS instances
    Type: AWS::EC2::KeyPair::KeyName
  VPCId:
    Description: Choose which VPC this ECS cluster should be deployed to
    Type: AWS::EC2::VPC::Id
  Subnets:
    Description: Choose which subnets this ECS cluster should be deployed to
    Type: List<AWS::EC2::Subnet::Id>
  BastionSG:
    Description: Security group used to whitelist for SSH access
    Type: AWS::EC2::SecurityGroup::Id
  ELBSecurityGroupId:
    Description: Security group used to whitelist elb access
    Type: String
    Default: ''
  DomainName:
    Description: >
      The domain under which the Kafka and Zookeeper nodes will register their advertised address.
      Note: This is the HostedZoneName without the period suffix.
    Type: String

  # Instances
  InstanceType:
    Description: Which instance type should we use to build the ECS cluster?
    Type: String
    AllowedValues: [
      m3.large, m3.xlarge, m3.2xlarge,
      m4.large, m4.xlarge, m4.2xlarge,
      m5.large, m5.xlarge, m5.2xlarge,
      c3.large, c3.xlarge, c3.2xlarge,
      c4.large, c4.xlarge, c4.2xlarge,
      c5.large, c5.xlarge, c5.2xlarge,
      r3.large, r3.xlarge, r3.2xlarge,
      r4.large, r4.xlarge, r4.2xlarge,
    ]
    Default: m5.large
    ConstraintDescription: Must be a valid EC2 instance type
  AdditionalSecurityGroup:
    Type: String
    Description: Additional security group to add ECS instances to
    Default: ''

  # Storage
  EBSMountPath:
    Description: Path at which to create the mount for the EBS data volume
    Type: String
    Default: /mnt/data
    MinLength: 1
    ConstraintDescription: Must be a valid folder path

  # ASG
  ASGEventsTopic:
    Description: SNS topic to notify of ASG events
    Type: String
  MaxInstancesPerAsg:
    Description: >
      Maximum number of ECS instances. This controls how we can add more capacity and should be 1 more
      than intended maximum so there is room to do rolling deployment.
    Type: Number
    Default: 2
    MinValue: 2
    MaxValue: 5
    ConstraintDescription: Must be a number between 2 and 5.

  # Features
  HostedZoneId:
    Description: The ID of the Hosted Zone in which to register services for service discovery
    Type: String
  KinesisStackName:
    Type: String
    Description: Name of the CF stack used to create the Kinesis stream
  SetupBrokerCerts:
    Type: String
    Default: 'no'
    AllowedValues: ['yes','no']
    Description: "If the cluster is being used for Kafka brokers use 'yes'; otherwise 'no'."
  NodeCount:
    Type: Number
    MinValue: 1
    Default: 3
    Description: The total number of nodes in the cluster.
    ConstraintDescription: Must be a value >= 1

  # Parameter to force a rolling deployment
  RollingDeploymentVersion:
    Type: Number
    Description: >
      Change this value to force a rolling deployment when there are no changes to
      the launch configuration. Its best to just increment it from the previous value.
    MinValue: 0
    Default: 0
    ConstraintDescription: Must be a non-negative number

  # Tags
  Project:
    Description: Project tag
    Type: String
    MinLength: 1
  Team:
    Description: Team tag
    Type: String
    MinLength: 1
  Environment:
    Description: Environment (dev|sandbox|prod)
    Type: String
    AllowedValues: ['dev','sandbox','prod']
    Default: dev

  ParentStackName:
    Description: Name of parent stack
    Type: String

  VolumeNameTag:
    Type: String
    Description: Tag for identifying the data volume to use for the ECS instances

  PrivateCaArn:
    Type: String
    Description: the ARN of the Private CA that signs the broker certificate

Mappings:
  # These are the latest ECS optimized AMIs as of March 2018:
  #
  #   amzn-ami-2017.09.l-amazon-ecs-optimized
  #   ECS agent:    1.18.0
  #   Docker:       17.12.1-ce
  #   ecs-init:     1.18.0-1
  #
  # You can find the latest available on this page of our documentation:
  # http://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-optimized_AMI.html
  # (note the AMI identifier is region specific)
  AWSRegionToAMI:
    us-east-1:
      AMI: ami-5253c32d
    ca-central-1:
      AMI: ami-da6cecbe

Resources:
  ECSLogGroup:
    Type: 'AWS::Logs::LogGroup'
    Properties:
      LogGroupName: !Ref 'AWS::StackName'
      RetentionInDays: 3
  SubscriptionFilter:
    Type: AWS::Logs::SubscriptionFilter
    DependsOn:
      - ECSLogGroup
    Properties:
      RoleArn: !ImportValue
        Fn::Sub: ${KinesisStackName}-Role-Arn
      LogGroupName: !Ref 'ECSLogGroup'
      FilterPattern: ''
      DestinationArn: !ImportValue
        Fn::Sub: ${KinesisStackName}-Stream-Arn

  ECSCluster:
    Type: AWS::ECS::Cluster
    Properties:
      ClusterName: !Ref 'AWS::StackName'

  # This IAM Role is attached to all of the ECS hosts. It is based on the default role published here:
  # http://docs.aws.amazon.com/AmazonECS/latest/developerguide/instance_IAM_role.html
  #
  # You can add other IAM policy statements here to allow access from your ECS hosts
  # to other AWS services. Please note that this role will be used by ALL containers
  # running on the ECS host.
  ECSInstanceRole:
    Type: AWS::IAM::Role
    Properties:
      Path: /
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Action: 'sts:AssumeRole'
          Effect: Allow
          Principal:
            Service: ec2.amazonaws.com
      ManagedPolicyArns: [
        'arn:aws:iam::aws:policy/service-role/AmazonEC2ContainerServiceforEC2Role',
        'arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess'
      ]
      Policies:
      - PolicyName: ECS-Service
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
          - Action: [
              'cloudwatch:PutMetricData',
              'cloudwatch:ListMetrics',
              'cloudwatch:GetMetricStatistics',
              'ds:CreateComputer',
              'ds:DescribeDirectories',
              'ec2:DescribeInstanceStatus',
              'ec2messages:AcknowledgeMessage',
              'ec2messages:DeleteMessage',
              'ec2messages:FailMessage',
              'ec2messages:GetEndpoint',
              'ec2messages:GetMessages',
              'ec2messages:SendReply',
              'ecs:DescribeClusters',
              'ecs:DescribeContainerInstances',
              'ecs:DescribeServices',
              'ecs:DescribeTaskDefinition',
              'ecs:DescribeTasks',
              'logs:CreateLogGroup',
              'logs:DescribeLogGroups',
              'logs:DescribeLogStreams',
              'route53:GetHostedZone',
              'route53:ListHostedZones',
              'route53:ChangeResourceRecordSets',
              'route53:ListResourceRecordSets',
              'route53:ListHostedZonesByName',
              's3:PutObject',
              's3:AbortMultipartUpload',
              's3:GetObject',
              'ssm:DescribeAssociation',
              'ssm:GetDeployablePatchSnapshotForInstance',
              'ssm:GetDocument',
              'ssm:GetManifest',
              'ssm:GetParameters',
              'ssm:ListAssociations',
              'ssm:ListInstanceAssociations',
              'ssm:PutInventory',
              'ssm:PutComplianceItems',
              'ssm:PutConfigurePackageResult',
              'ssm:UpdateAssociationStatus',
              'ssm:UpdateInstanceAssociationStatus',
              'ssm:UpdateInstanceInformation',
              'ec2:DescribeInstances',
              'ec2:DescribeTags',
              'ec2:CreateTags',
              'ec2:DescribeVolumes',
              'ec2:AttachVolume',
              'acm-pca:GetCertificate',
              'acm-pca:GetCertificateAuthorityCertificate',
              'acm-pca:IssueCertificate'
            ]
            Effect: Allow
            Resource: '*'

  ECSInstanceProfile:
    Type: AWS::IAM::InstanceProfile
    Properties:
      Path: /
      Roles:
        - !Ref 'ECSInstanceRole'

  ECSSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      VpcId: !Ref 'VPCId'
      GroupDescription: Allow connections from vpc
      SecurityGroupIngress:
      - SourceSecurityGroupId: !Ref 'BastionSG'
        IpProtocol: tcp
        FromPort: 22
        ToPort: 22
      Tags:
      - Key: Name
        Value: !Sub 'ECS Instance SG - ${AWS::StackName}'
      - Key: Project
        Value: !Ref 'Project'
      - Key: Team
        Value: !Ref 'Team'
      - Key: Environment
        Value: !Ref 'Environment'
      - Key: Component
        Value: ecs
      SecurityGroupEgress:
        IpProtocol: -1
        CidrIp: "0.0.0.0/0"

  ECSLaunchConfiguration:
    Type: AWS::AutoScaling::LaunchConfiguration
    Properties:
      AssociatePublicIpAddress: false
      EbsOptimized: true
      IamInstanceProfile: !Ref 'ECSInstanceProfile'
      ImageId:  !FindInMap [AWSRegionToAMI, !Ref 'AWS::Region', AMI]
      InstanceType: !Ref 'InstanceType'
      KeyName: !Ref 'KeyName'
      SecurityGroups:
        - !Ref 'ECSSecurityGroup'
        - !Ref 'AdditionalSecurityGroup'
      UserData:
        'Fn::Base64': !Sub |
          #!/bin/bash -e

          exec >> /var/log/user-data.log 2>&1

          # Increment RollingDeploymentVersion parameter to force rolling deploy
          version=${RollingDeploymentVersion}
          echo $version

          yum install -y https://s3.amazonaws.com/ec2-downloads-windows/SSMAgent/latest/linux_amd64/amazon-ssm-agent.rpm
          yum install -y aws-cfn-bootstrap

          echo "Initializing Instance"
          config_set="general"

          /opt/aws/bin/cfn-init -v --region ${AWS::Region} --stack ${AWS::StackName} --resource ECSLaunchConfiguration --configsets $config_set
          exit_code=$?

          if [ $exit_code == 0 ]; then
            instance_id=$(curl -s http://169.254.169.254/latest/meta-data/instance-id)

            echo "Grabbing tag of autoscaling resource name with instance id='$instance_id'"
            asg_resource_name=$(aws ec2 describe-tags --region ${AWS::Region} --filters Name=resource-id,Values=$instance_id Name=tag-key,Values=AsgResourceName | jq -r '.Tags[0].Value')
          fi

          echo "Signalling ASG with resource name='$asg_resource_name'"
          /opt/aws/bin/cfn-signal -e $exit_code --region ${AWS::Region} --stack ${AWS::StackName} --resource $asg_resource_name
    Metadata:
      AWS::CloudFormation::Init:
        configSets:
          general:
            - install_logging
            - install_ecs_instance
            - install_monitoring
            - install_service_discovery

        # Configure and install aws logs agent
        install_logging:
          packages:
            yum:
              awslogs: []
              jq: []
          files:
            '/etc/awslogs/awscli.conf':
              content: !Sub |
                [plugins]
                cwlogs = cwlogs
                [default]
                region = ${AWS::Region}
            '/etc/awslogs/awslogs.conf':
              content: !Sub |
                [general]
                state_file = /var/lib/awslogs/agent-state

                [/var/log/dmesg]
                file = /var/log/dmesg
                log_group_name = {cluster}
                log_stream_name = {instance_id}/dmesg

                [/var/log/messages]
                file = /var/log/messages
                log_group_name = {cluster}
                log_stream_name = {instance_id}/messages
                datetime_format = %b %d %H:%M:%S

                [/var/log/docker]
                file = /var/log/docker
                log_group_name = {cluster}
                log_stream_name = {instance_id}/docker
                datetime_format = %Y-%m-%dT%H:%M:%S.%f

                [/var/log/ecs/ecs-init.log]
                file = /var/log/ecs/ecs-init.log
                log_group_name = {cluster}
                log_stream_name = {instance_id}/ecs-init.log
                datetime_format = %Y-%m-%dT%H:%M:%SZ

                [/var/log/ecs/ecs-agent.log]
                file = /var/log/ecs/ecs-agent.log.*
                log_group_name = {cluster}
                log_stream_name = {instance_id}/ecs-agent.log
                datetime_format = %Y-%m-%dT%H:%M:%SZ

                [/var/log/cloud-init.log]
                file = /var/log/cloud-init.log
                log_group_name = {cluster}
                log_stream_name = {instance_id}/cloud-init.log
                datetime_format = %Y-%m-%dT%H:%M:%SZ

                [/var/log/cloud-init-output.log]
                file = /var/log/cloud-init-output.log
                log_group_name = {cluster}
                log_stream_name = {instance_id}/cloud-init-output.log
                datetime_format = %Y-%m-%dT%H:%M:%SZ

                [/var/log/cfn-init.log]
                file = /var/log/cfn-init.log
                log_group_name = {cluster}
                log_stream_name = {instance_id}/cfn-init.log
                datetime_format = %Y-%m-%dT%H:%M:%SZ

                [/var/log/cfn-init-cmd.log]
                file = /var/log/cfn-init-cmd.log
                log_group_name = {cluster}
                log_stream_name = {instance_id}/cfn-init-cmd.log
                datetime_format = %Y-%m-%dT%H:%M:%SZ

                [/var/log/user-data.log]
                file = /var/log/user-data.log
                log_group_name = {cluster}
                log_stream_name = {instance_id}/user-data.log
                datetime_format = %Y-%m-%dT%H:%M:%SZ

                [/var/log/awslogs-config.log]
                file = /var/log/awslogs-config.log
                log_group_name = {cluster}
                log_stream_name = {instance_id}/awslogs-config.log
                datetime_format = %Y-%m-%dT%H:%M:%SZ

                [/var/log/service-discovery.log]
                file = /var/log/service-discovery.log
                log_group_name = {cluster}
                log_stream_name = {instance_id}/service-discovery.log
                datetime_format = %Y-%m-%dT%H:%M:%SZ
            '/etc/init/awslogs.conf':
              content: |
                #upstart-job
                description "Configure and start CloudWatch Logs agent on Amazon ECS container instance"
                author "Amazon Web Services"
                start on started ecs

                script
                  exec >> /var/log/awslogs-config.log 2>&1
                  set -e
                  set -x

                  until curl -s http://localhost:51678/v1/metadata
                  do
                      sleep 1
                  done

                  cluster=$(curl -s http://localhost:51678/v1/metadata | jq -r '. | .Cluster')
                  sed -i -e "s/{cluster}/$cluster/g" /etc/awslogs/awslogs.conf

                  container_instance_id=$(curl -s http://localhost:51678/v1/metadata | jq -r '. | .ContainerInstanceArn' | awk -F/ '{print $2}')
                  sed -i -e "s/{container_instance_id}/$container_instance_id/g" /etc/awslogs/awslogs.conf

                  region=$(curl 169.254.169.254/latest/meta-data/placement/availability-zone | sed s'/.$//')
                  sed -i -e "s/region = us-east-1/region = $region/g" /etc/awslogs/awscli.conf

                  service awslogs start
                  chkconfig awslogs on
                end script

        install_ecs_instance:
          files:
            '/usr/local/share/setup-tags':
              mode: '000744'
              owner: root
              group: root
              content: !Sub |
                #!/bin/bash -e

                # Tag this instance's related resources before attaching other resources that might have tags
                # we shouldn't override.

                exec >> /var/log/user-data.log 2>&1

                # Setup environment variables
                export AWS_DEFAULT_REGION=${AWS::Region}

                instance_id=$(curl -s http://169.254.169.254/latest/meta-data/instance-id)
                instance_data=$(aws ec2 describe-instances --instance-id $instance_id)
                instance_name=$(echo "$instance_data" | jq -r '.Reservations[0].Instances[0].Tags[] | select(.Key=="Name").Value')
                network_ids=($(echo $instance_data | jq -r '.Reservations[0].Instances[0].NetworkInterfaces[].NetworkInterfaceId'))
                volume_ids=($(echo $instance_data | jq -r '.Reservations[0].Instances[0].BlockDeviceMappings[].Ebs.VolumeId'))

                echo "Setting up tags for '$instance_id'; resources=[${!volume_ids[@]} ${!network_ids[@]}]..."
                tags="'Key=Name,Value=\"${!instance_name}\"' 'Key=Project,Value=${Project}' 'Key=Team,Value=${Team}' 'Key=Environment,Value=${Environment}'"
                eval aws ec2 create-tags --resources ${!volume_ids[@]} ${!network_ids[@]} --tags $tags
            '/usr/local/share/setup-data-volume':
              mode: '000744'
              owner: root
              group: root
              content: !Sub |
                #!/bin/bash -e

                exec >> /var/log/user-data.log 2>&1

                # Setup environment variables

                export instance_id=$(curl -s http://169.254.169.254/latest/meta-data/instance-id)
                export az=$(curl -s http://169.254.169.254/latest/meta-data/placement/availability-zone)
                export AWS_DEFAULT_REGION=${AWS::Region}

                # This script will find next available volume in the instance's availability zone and attach it. It will
                # initialize the file system if it is the first time it attaches; this is determined by existence of the
                # InstanceId tag (initially it is not present). Finally it will mount the file system and add the
                # InstanceId tag of current instance.

                # Find a volume in the availability zone to attach with Name ${ParentStackName}-${VolumeNameTag}
                status=
                volume_id=
                while [[ "$status" != "attached" ]]; do
                  echo "Searching for available volumes in '$az'"
                  ids=($(aws ec2 describe-volumes --filters Name=status,Values=available Name=availability-zone,Values=$az Name=tag-key,Values=Name Name=tag-value,Values=${ParentStackName}-${VolumeNameTag} | jq -r '.Volumes[].VolumeId'))

                  if [[ ${!#ids[@]} -gt 0 ]]; then
                    # Try first volume. If for some reason there is a race condition, next time around we try it should pick
                    # a different available volume. Number of volumes match the ensemble and availability zones so unless
                    # something really bad happened, this should succeed at most once.
                    echo "Attaching volume id ${!ids[0]}..."
                    status=$(aws ec2 attach-volume --device /dev/xvdh --instance-id $instance_id --volume-id ${!ids[0]} | jq -r '.State')

                    if [[ -z $status ]]; then
                      echo "Waiting for data volumes to become available..."
                    elif [[ "$status" -eq "attaching" ]]; then
                      while [[ "$status" != "attached" ]]; do
                         echo "Waiting for data volume ${!ids[0]} to be attached..."
                         status=$(aws ec2 describe-volumes --volume-id ${!ids[0]} | jq -r '.Volumes[0].Attachments[0].State')
                         sleep 10
                      done

                      echo "Successfully attached data volume ${!ids[0]}"
                      volume_id=${!ids[0]}
                    elif [[ "$status" -eq "attached" ]]; then
                      echo "Successfully attached data volume ${!ids[0]}"
                      volume_id=${!ids[0]}
                    else
                      echo "Volume ${!ids[0]} status is '$status'. Waiting for volumes to become available..."
                      sleep 10
                    fi
                  else
                    echo "Waiting for volumes to become available..."
                    sleep 10
                  fi
                done

                # Check volume's tag to see if it belonged to another instance so we don't reformat it
                attached_instance_id=$(aws ec2 describe-tags --filters Name=resource-id,Values=$volume_id Name=tag-key,Values=InstanceId | jq -r '.Tags[].Value')
                if [[ -z $attached_instance_id ]]; then
                  echo "First time attachment. Creating XFS file system..."
                  mkfs.xfs -f /dev/xvdh
                fi

                # Setup and mount data volume
                echo "Setting up mount point for data volume..."
                mkdir -p ${EBSMountPath}

                echo "Mounting file system..."
                echo "/dev/xvdh  ${EBSMountPath}  xfs  defaults,inode64,nofail,context=system_u:object_r:var_t:s0  0 2" >> /etc/fstab
                mount -v ${EBSMountPath}

                # Add tag to the volume so we can identify ownership to the instance
                echo "Setting up tag InstanceId=$instance_id for volume $volume_id..."
                aws ec2 create-tags --resources $volume_id --tags Key=InstanceId,Value=$instance_id

                # Get NodeId from volume tag and transfer to instance
                node_id=$(aws ec2 describe-volumes --volume-ids $volume_id | jq -r '.Volumes[0].Tags[] | select(.Key == "NodeId").Value')
                echo "Setting up InstanceId=$instance_id for tag NodeId=$node_id..."
                aws ec2 create-tags --resources $instance_id --tags Key=NodeId,Value=$node_id
            '/usr/local/share/setup-ssl-certs':
              mode: '000744'
              owner: root
              group: root
              content: !Sub |
                #!/bin/bash -e

                exec >> /var/log/user-data.log 2>&1

                if [[ "${SetupBrokerCerts}" == "no" ]]; then
                  echo "Skipping Broker SSL configuration."
                  exit 0
                fi

                echo "Beginning Broker SSL configuration"

                cd /tmp/
                curl -O https://bootstrap.pypa.io/get-pip.py
                python /tmp/get-pip.py
                /usr/local/bin/pip install awscli --upgrade --user

                mkdir -p ${EBSMountPath}/ssl

                # Get CA certificate
                aws acm-pca get-certificate-authority-certificate --region ${AWS::Region} --certificate-authority-arn ${PrivateCaArn}  | jq -r '.Certificate' > ${EBSMountPath}/ssl/ca.crt

                # Generate SAN
                alt_names=""
                for i in $(seq 1 ${NodeCount}); do
                  alt_names=$(echo -e "${!alt_names}\nDNS.${!i} = ${ParentStackName}-broker${!i}.${DomainName}")
                done
                alt_names=$(echo "$alt_names" | sed -e '/^$/d')

                echo "The SAN configuration is:"
                echo -e "$alt_names"

                # Generate private key
                openssl req -new \
                  -sha256 \
                  -nodes \
                  -out ${EBSMountPath}/ssl/broker.csr \
                  -newkey rsa:2048 \
                  -keyout ${EBSMountPath}/ssl/broker.key \
                  -config <(
                cat <<-EOF
                [req]
                default_bits = 2048
                prompt = no
                default_md = sha256
                req_extensions = req_ext
                distinguished_name = dn

                [ dn ]
                C = CA
                ST = Ontario
                L = Toronto
                O = LoyaltyOne, Co.
                OU = IT
                CN = ${ParentStackName}-broker.${DomainName}

                [ req_ext ]
                subjectAltName = @alt_names

                [ alt_names ]
                $alt_names
                EOF
                )

                # have aws private ca issue cert
                certificate_arn=$(aws acm-pca issue-certificate --region ${AWS::Region} \
                  --certificate-authority-arn ${PrivateCaArn} \
                  --csr file://${EBSMountPath}/ssl/broker.csr \
                  --signing-algorithm "SHA256WITHRSA" \
                  --validity Value=365,Type="DAYS" \
                  --idempotency-token 1234 | jq -r '.CertificateArn')

                echo "Sleeping for 30 seconds while cert is issued"
                sleep 30
                echo "cert arn:" + $certificate_arn

                aws acm-pca get-certificate --certificate-authority-arn ${PrivateCaArn} --region ${AWS::Region} --certificate-arn $certificate_arn | jq -r '.Certificate' > ${EBSMountPath}/ssl/broker.crt

                # fetch loissuingca root cert
                aws s3 cp s3://dev-kafka-artifacts/certs/loissuingca.pem ${EBSMountPath}/ssl/loissuingca.pem --region us-east-1
                cat ${EBSMountPath}/ssl/loissuingca.pem >> ${EBSMountPath}/ssl/ca.crt

                # create PKCS12 format to allow importing into keystore
                openssl pkcs12 -export -in ${EBSMountPath}/ssl/broker.crt -inkey ${EBSMountPath}/ssl/broker.key -out ${EBSMountPath}/ssl/broker.p12 -name broker -CAfile ${EBSMountPath}/ssl/ca.crt -caname root -password pass:changeme
          commands:
            01_setup_tags:
              command: '/usr/local/share/setup-tags'
            02_setup_ebs_volume:
              command: '/usr/local/share/setup-data-volume'
            03_setup_ssl:
              command: '/usr/local/share/setup-ssl-certs'
            03_add_instance_to_cluster:
              # Let ecs agent know which cluster to join
              command: !Sub 'echo ECS_CLUSTER=${ECSCluster} >> /etc/ecs/ecs.config'

        install_service_discovery:
          files:
            "/etc/init/service_discovery.conf":
              mode: 000644
              owner: root
              group: root
              content: !Sub |
                #upstart-job
                description "Run service discovery agent after ecs is ready"
                author "LoyaltyOne"
                start on started ecs

                script
                  exec >> /var/log/service-discovery.log 2>&1
                  set -e
                  set -x

                  until curl -s http://localhost:51678/v1/metadata
                  do
                      sleep 1
                  done

                  cluster=$(curl -s http://localhost:51678/v1/metadata | jq -r '. | .Cluster')
                  region=$(curl 169.254.169.254/latest/meta-data/placement/availability-zone | sed s'/.$//')

                  # Pull docker image. Try up to 3 times before giving up.
                  docker_image=loyaltyone/service-discovery-agent-route53:latest
                  count=3
                  while [[ $count -gt 0 ]]; do
                    echo "Fetching docker image '$docker_image'..."
                    docker pull $docker_image
                    if [[ "$?" -gt 0 ]]; then
                      count=$(($count - 1))
                      echo "Problem fetching docker image '$docker_image'."
                      echo "Taking a nap for 5 seconds..."
                      sleep 5
                    else
                      echo "Fetched docker image for '$docker_image'."
                      count=0
                    fi
                  done

                  # Start service discovery agent
                  echo "Running service discovery agent..."
                  docker run -d \
                    --name service-discovery-agent \
                    --restart always \
                    -v /var/run/docker.sock:/var/run/docker.sock \
                    --add-host host:$(ifconfig docker0 | grep -oP 'inet addr:\K\S+') \
                    --log-driver=awslogs \
                    --log-opt awslogs-group=$cluster \
                    --log-opt awslogs-region=$region \
                    --log-opt tag='{{.Name}}/{{.ID}}' \
                    $docker_image --hostedZoneId=${HostedZoneId}
                end script
        install_monitoring:
          packages:
            yum:
              unzip: []
          commands:
            01_install_disk_mem_monitoring:
              command: |
                #!/bin/bash -e

                exec >> /var/log/user-data.log 2>&1
                set -e
                set -x

                yum install perl-Switch perl-DateTime perl-Sys-Syslog perl-LWP-Protocol-https perl-Digest-SHA.x86_64 -y

                version=1.2.2
                archive=CloudWatchMonitoringScripts-$version.zip

                mon_dir=/opt/monitoring

                mkdir -p $mon_dir
                cd $mon_dir
                curl http://aws-cloudwatch.s3.amazonaws.com/downloads/$archive -O

                unzip $archive
                rm $archive
          files:
            "/etc/cron.d/metrics":
              content: !Sub |
                # Version: 1.1.2-rpm
                PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin

                * * * * * root /opt/monitoring/aws-scripts-mon/mon-put-instance-data.pl --mem-used-incl-cache-buff --mem-avail --mem-util --mem-used --disk-space-units=bytes --disk-space-util --disk-space-used --disk-space-avail --disk-path=/ --disk-path=${EBSMountPath} --auto-scaling --from-cron

              mode: '000644'
              owner: root
              group: root

  ECSAutoScalingGroup0:
    DependsOn: 'ECSLogGroup'
    Type: AWS::AutoScaling::AutoScalingGroup
    Properties:
      VPCZoneIdentifier: [!Select [0, !Ref 'Subnets']]
      LaunchConfigurationName: !Ref 'ECSLaunchConfiguration'
      MinSize: 1
      MaxSize: !Ref 'MaxInstancesPerAsg'
      DesiredCapacity: 1
      NotificationConfigurations:
        - TopicARN: !Ref 'ASGEventsTopic'
          NotificationTypes:
          - autoscaling:EC2_INSTANCE_TERMINATE
      Tags:
        - Key: Name
          Value: !Sub 'ECS Container Instance - ${AWS::StackName}'
          PropagateAtLaunch: true
        - Key: Project
          Value: !Ref 'Project'
          PropagateAtLaunch: true
        - Key: Team
          Value: !Ref 'Team'
          PropagateAtLaunch: true
        - Key: Environment
          Value: !Ref 'Environment'
          PropagateAtLaunch: true
        - Key: Component
          Value: ecs
          PropagateAtLaunch: true
        - Key: Scheduler
          Value: false
          PropagateAtLaunch: true
        - Key: AsgResourceName
          Value: 'ECSAutoScalingGroup0'
          PropagateAtLaunch: true
    CreationPolicy:
      ResourceSignal:
        Timeout: PT25M
    UpdatePolicy:
      AutoScalingRollingUpdate:
        MinInstancesInService: 1
        MaxBatchSize: 1
        PauseTime: PT15M
        SuspendProcesses:
          - HealthCheck
          - ReplaceUnhealthy
          - AZRebalance
          - AlarmNotification
          - ScheduledActions
        WaitOnResourceSignals: true
  ECSAutoScalingGroup1:
    DependsOn: 'ECSLogGroup'
    Type: AWS::AutoScaling::AutoScalingGroup
    Properties:
      VPCZoneIdentifier: [!Select [1, !Ref 'Subnets']]
      LaunchConfigurationName: !Ref 'ECSLaunchConfiguration'
      MinSize: 1
      MaxSize: !Ref 'MaxInstancesPerAsg'
      DesiredCapacity: 1
      NotificationConfigurations:
        - TopicARN: !Ref 'ASGEventsTopic'
          NotificationTypes:
          - autoscaling:EC2_INSTANCE_TERMINATE
      Tags:
        - Key: Name
          Value: !Sub 'ECS Container Instance - ${AWS::StackName}'
          PropagateAtLaunch: true
        - Key: Project
          Value: !Ref 'Project'
          PropagateAtLaunch: true
        - Key: Team
          Value: !Ref 'Team'
          PropagateAtLaunch: true
        - Key: Environment
          Value: !Ref 'Environment'
          PropagateAtLaunch: true
        - Key: Component
          Value: ecs
          PropagateAtLaunch: true
        - Key: Scheduler
          Value: false
          PropagateAtLaunch: true
        - Key: AsgResourceName
          Value: 'ECSAutoScalingGroup1'
          PropagateAtLaunch: true
    CreationPolicy:
      ResourceSignal:
        Timeout: PT25M
    UpdatePolicy:
      AutoScalingRollingUpdate:
        MinInstancesInService: 1
        MaxBatchSize: 1
        PauseTime: PT15M
        SuspendProcesses:
          - HealthCheck
          - ReplaceUnhealthy
          - AZRebalance
          - AlarmNotification
          - ScheduledActions
        WaitOnResourceSignals: true
  ECSAutoScalingGroup2:
    DependsOn: 'ECSLogGroup'
    Type: AWS::AutoScaling::AutoScalingGroup
    Properties:
      VPCZoneIdentifier: [!Select [2, !Ref 'Subnets']]
      LaunchConfigurationName: !Ref 'ECSLaunchConfiguration'
      MinSize: 1
      MaxSize: !Ref 'MaxInstancesPerAsg'
      DesiredCapacity: 1
      NotificationConfigurations:
        - TopicARN: !Ref 'ASGEventsTopic'
          NotificationTypes:
          - autoscaling:EC2_INSTANCE_TERMINATE
      Tags:
        - Key: Name
          Value: !Sub 'ECS Container Instance - ${AWS::StackName}'
          PropagateAtLaunch: true
        - Key: Project
          Value: !Ref 'Project'
          PropagateAtLaunch: true
        - Key: Team
          Value: !Ref 'Team'
          PropagateAtLaunch: true
        - Key: Environment
          Value: !Ref 'Environment'
          PropagateAtLaunch: true
        - Key: Component
          Value: ecs
          PropagateAtLaunch: true
        - Key: Scheduler
          Value: false
          PropagateAtLaunch: true
        - Key: AsgResourceName
          Value: 'ECSAutoScalingGroup2'
          PropagateAtLaunch: true
    CreationPolicy:
      ResourceSignal:
        Timeout: PT25M
    UpdatePolicy:
      AutoScalingRollingUpdate:
        MinInstancesInService: 1
        MaxBatchSize: 1
        PauseTime: PT15M
        SuspendProcesses:
          - HealthCheck
          - ReplaceUnhealthy
          - AZRebalance
          - AlarmNotification
          - ScheduledActions
        WaitOnResourceSignals: true

  ECSAutoScalingNotificationAccessRole:
    Type: AWS::IAM::Role
    Properties:
      Path: /
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Action: 'sts:AssumeRole'
            Effect: Allow
            Principal:
              Service: autoscaling.amazonaws.com
      ManagedPolicyArns: ['arn:aws:iam::aws:policy/service-role/AutoScalingNotificationAccessRole']

  ECSAutoScalingGroupLifecycleHook0:
    Type: AWS::AutoScaling::LifecycleHook
    Properties:
      AutoScalingGroupName: !Ref 'ECSAutoScalingGroup0'
      LifecycleTransition: 'autoscaling:EC2_INSTANCE_TERMINATING'
      NotificationTargetARN: !Ref 'ASGEventsTopic'
      RoleARN: !GetAtt 'ECSAutoScalingNotificationAccessRole.Arn'
      DefaultResult: 'ABANDON'
      HeartbeatTimeout: '300'
  ECSAutoScalingGroupLifecycleHook1:
    Type: AWS::AutoScaling::LifecycleHook
    Properties:
      AutoScalingGroupName: !Ref 'ECSAutoScalingGroup1'
      LifecycleTransition: 'autoscaling:EC2_INSTANCE_TERMINATING'
      NotificationTargetARN: !Ref 'ASGEventsTopic'
      RoleARN: !GetAtt 'ECSAutoScalingNotificationAccessRole.Arn'
      DefaultResult: 'ABANDON'
      HeartbeatTimeout: '300'
  ECSAutoScalingGroupLifecycleHook2:
    Type: AWS::AutoScaling::LifecycleHook
    Properties:
      AutoScalingGroupName: !Ref 'ECSAutoScalingGroup2'
      LifecycleTransition: 'autoscaling:EC2_INSTANCE_TERMINATING'
      NotificationTargetARN: !Ref 'ASGEventsTopic'
      RoleARN: !GetAtt 'ECSAutoScalingNotificationAccessRole.Arn'
      DefaultResult: 'ABANDON'
      HeartbeatTimeout: '300'

Outputs:
  AsgName0:
    Description: Name of first ASG created
    Value: !Ref 'ECSAutoScalingGroup0'
  AsgName1:
    Description: Name of second ASG created
    Value: !Ref 'ECSAutoScalingGroup1'
  AsgName2:
    Description: Name of third ASG created
    Value: !Ref 'ECSAutoScalingGroup2'
  ClusterName:
    Description: A reference to the ECS cluster
    Value: !Ref 'ECSCluster'
  InstanceSecurityGroup:
    Description: The ID of the ecs container instance security group created
    Value: !GetAtt [ECSSecurityGroup, GroupId]
  VpcId:
    Description: "Cluster's VPC Id"
    Value: !Ref 'VPCId'
  EBSMountPath:
    Description: Path at which to create the mount for the EBS data volume
    Value: !Ref 'EBSMountPath'
